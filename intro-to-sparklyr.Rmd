---
title: "Sparklyr Introduction"
output: html_notebook
---

In this notebook we walk through some basic examples of how to use sparklyr in R. This notebook is based on the guide provided at http://spark.rstudio.com/. 

## Connecting to Spark
First we load **sparklyr** and install Spark if it is not already installed:
```{r}
library(sparklyr)
spark_install(version = "2.3.1")
```
We can connect to clusters and local instances of Spark as follows (In RStudio make sure to create a new Spark connection):
```{r}
sc <- spark_connect(master = "local")
```

## Using dplyr
Next we will show that we can use all of the standard dplyr verbs against tables in the cluster. First we must populate the cluster with data:
```{r}
library(dplyr)
iris_tbl <- copy_to(sc, iris)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
batting_tbl <- copy_to(sc, Lahman::Batting, "batting")
src_tbls(sc)
```
Visiting the SparkUI we see that this is a relatively small database (only about 50MB). We could imagine in real cases we would be working with larger databases. Note that the data tables lie `iris_tbl` are lists and are not stored in memory.

Now let's try a simple filtering example:
```{r}
# filter by departure delay and print the first few records
flights_tbl %>% filter(dep_delay == 2) %>% head()
```
Let's quickly review some dplyr grammar: The pipe operator `%>%` indicates feeding the expression on its left-hand side into the function on the right-hand side. This allows us to chain many functions on a dataframe while maintaining readability of the command (compare the above with the equivalent expression `head(filter(flights_tbl, dep_delay==2))`).

A more coplicated example:
```{r}
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect
```
Plot the queried data:
```{r}
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```
Now we review ggplot2 syntax: The first line specifies the x-y axes of the plot. Each additional line preceded by a '+' indicates another layer to be added. The first layer added is a scatter plot where the point size is determined by the counts of instances. The second layer is a smoothed conditional mean with standard errors. The conditional mean is modeled by a generalized additive model (GAM) and the confidence intervals are determined by asymptotics (I assume;  verify this). We will not take the smoothing function too seriously, it is simply here to roughly illustrate the conditional mean.

## Using SQL
We can similarly use SQL statements to query data in the cluster. The `spark_connection` object `sc` implements a **DBI** interface for Spark so we use `dbGetQuery()`:
```{r SQL Query} 
library("DBI")
iris_preview <- dbGetQuery(sc, "SELECT * FROM iris LIMIT 10")
iris_preview
```

## Machine Learning
Sparklyr comes prepackaged with the machine learning algorithms from **spark.ml** that can be used on the `spark_connection` object. Here we will demonstrate a simple linear regression model `ml_linear_regression` on the *mtcars* dataset to model fuel consumption as a function of a vehicle's weight and cylinders. First we copy the dataset into Spark:
```{r Copying mtcars}
mtcars_tbl = copy_to(sc, mtcars)
```
We perform some pre-processing and partition the data into equally-sized training and test set using **dplyr** verbs (Note that this operation creates datasets in memory!):
```{r Partitioning Data}
partitions <- mtcars_tbl %>%
  filter(hp >= 100) %>%
  mutate(cyl8 = cyl == 8) %>%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)
```
Next we fit the model. Note that the **dplyr** pipe syntax can be used.
```{r Model Fit}
fit <- partitions$training %>%
  ml_linear_regression(response = "mpg", features = c("wt", "cyl"))
print(fit)
```
The `summary()` function provides an `lm()`-like summary output:
```{r}
summary(fit)
```